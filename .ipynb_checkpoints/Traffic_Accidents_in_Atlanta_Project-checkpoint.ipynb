{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d832d623",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\ricky\\\\Downloads\\\\traffic_accidents_atlanta.csv\\\\traffic_accidents_atlanta.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#step 1: Load the dataset:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m Atl_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mricky\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtraffic_accidents_atlanta.csv\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mtraffic_accidents_atlanta.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#dropping rows that contain Atlanta but are not from Georgia, USA\u001b[39;00m\n\u001b[1;32m     10\u001b[0m Atl_data\u001b[38;5;241m.\u001b[39mdrop(Atl_data[Atl_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGA\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mindex, inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ricky\\\\Downloads\\\\traffic_accidents_atlanta.csv\\\\traffic_accidents_atlanta.csv'"
     ]
    }
   ],
   "source": [
    "#step 1: Load the dataset:\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Atl_data = pd.read_csv(r\"C:\\Users\\ricky\\Downloads\\traffic_accidents_atlanta.csv\\traffic_accidents_atlanta.csv\")\n",
    "\n",
    "#dropping rows that contain Atlanta but are not from Georgia, USA\n",
    "Atl_data.drop(Atl_data[Atl_data['State'] != 'GA'].index, inplace = True)\n",
    "\n",
    "#dropping the 'Number column' due to dataset author request\n",
    "Atl_data.drop(columns = ['Number'], inplace = True)\n",
    "\n",
    "                         \n",
    "#dropping unnamed column in the dataset that we have no information about\n",
    "Atl_data = Atl_data.drop(\"Unnamed: 0\", axis = 1)\n",
    "                         \n",
    "                         \n",
    "#target variable for our project\n",
    "print(Atl_data['Severity'].unique())\n",
    "\n",
    "\n",
    "\n",
    "print(Atl_data.head())\n",
    "print(Atl_data.describe())\n",
    "print(Atl_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77dbab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: Data preprocessing and exploration:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "#handling missing values using mean for numerical columns/features\n",
    "numeric_features = Atl_data.select_dtypes(include = np.number).columns\n",
    "impute_mean = SimpleImputer(strategy = 'mean')\n",
    "Atl_data_numeric = pd.DataFrame(impute_mean.fit_transform(Atl_data[numeric_features]), columns = numeric_features)\n",
    "\n",
    "\n",
    "#handling missing values using mode for categorical columns/features\n",
    "categorical_features = Atl_data.select_dtypes(include = 'object').columns\n",
    "impute_mode = SimpleImputer(strategy = 'most_frequent')\n",
    "Atl_data_categorical = pd.DataFrame(impute_mode.fit_transform(Atl_data[categorical_features]), columns = categorical_features)\n",
    "\n",
    "\n",
    "#one-hot encoding for categorical features.... RF classifier expects numeric input, but our dataset still contains categorical features... we need to convert the categorical features into numeric form using one-hot encoding\n",
    "Atl_data_categorical_encoded = pd.get_dummies(Atl_data_categorical)\n",
    "\n",
    "\n",
    "#Combining the imputed numeric and (imputed and encoded) categorical data\n",
    "Atl_data_combined = pd.concat([Atl_data_numeric, Atl_data_categorical_encoded], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "#handling outliers using IQR for numeric columns/features\n",
    "Q1 = Atl_data_combined.quantile(0.25)\n",
    "Q3 = Atl_data_combined.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "#removing the outliers\n",
    "Atl_data_n = Atl_data_combined[(Atl_data_combined >= lower_bound) & (Atl_data_combined <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "#Normalizing numeric columns/features (excluding the target variable 'Severity') data using range normalization (MinMaxScaler)\n",
    "Normalizing_Features = Atl_data_n.drop('Severity', axis = 1).columns\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = pd.DataFrame(scaler.fit_transform(Atl_data_n[Normalizing_Features]), columns = Normalizing_Features)\n",
    "\n",
    "\n",
    "#combining normalized features with the original 'Severity' column\n",
    "normalized_Atl_data = pd.concat([Atl_data_n['Severity'], normalized_features], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "#Replacing NaN or infinite values with the mean of the column\n",
    "normalized_Atl_data = normalized_Atl_data.apply(lambda x: x.replace([np.inf, -np.inf], np.nan))\n",
    "normalized_Atl_data.fillna(normalized_Atl_data.mean(), inplace = True)\n",
    "\n",
    "\n",
    "#Feature selection using Random Forest\n",
    "#target feature is 'Severity', it represents the severity of the accident based on 4 classes (1, 2, 3, 4) where 1 is the low severity and 4 is the highest severity\n",
    "X = normalized_Atl_data.drop('Severity', axis = 1)\n",
    "y = normalized_Atl_data['Severity']\n",
    "\n",
    "\n",
    "#intializing the random forest model\n",
    "rf_model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "#train the random forest model\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "#get the importance of features\n",
    "feature_importances = pd.Series(rf_model.feature_importances_, index = X.columns)\n",
    "\n",
    "\n",
    "#select the most important features\n",
    "selected_features = feature_importances[feature_importances > 0.01]\n",
    "columns_selection = selected_features.index\n",
    "\n",
    "X_selected = X[columns_selection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e12dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 3: preparing data for machine learning models:\n",
    "#train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a66ac7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#step 4: Model Training\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "#addressing NaN or infinity values in the training data\n",
    "if X_train.isnull().sum().sum() > 0:\n",
    "    X_train.fillna(X_train.mean(), inplace = True)\n",
    "\n",
    "if y_train.isnull().sum() > 0:\n",
    "    y_train.fillna(y_train.mode()[0], inplace = True)\n",
    "    \n",
    "\n",
    "#Intialize the models (gradient boosting classifier, random forest classifier, and logistic regression model)\n",
    "logistic_reg_model = LogisticRegression(solver = 'lbfgs', max_iter = 1000, random_state = 42)\n",
    "Gradient_Boo_model = GradientBoostingClassifier(random_state = 42)\n",
    "Random_For_model = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "\n",
    "#standardize the data\n",
    "scaler_std = StandardScaler()\n",
    "X_train_std = scaler_std.fit_transform(X_train)\n",
    "X_test_std = scaler_std.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#addressing Nan or infinity values in the test data\n",
    "if np.isnan(X_test_std).sum() > 0:\n",
    "    X_test_std[np.isnan(X_test_std)] = np.nanmean(X_test_std)\n",
    "\n",
    "X_test_std = np.nan_to_num(X_test_std, nan = 0.0, posinf = None, neginf = None)\n",
    "    \n",
    "    \n",
    "#Training the models\n",
    "logistic_reg_model.fit(X_train_std, y_train)\n",
    "Gradient_Boo_model.fit(X_train_std, y_train)\n",
    "Random_For_model.fit(X_train_std, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35902917",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#step 5: model evalutaion\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "#making prediction using the models\n",
    "logistic_reg_preds = logistic_reg_model.predict(X_test_std)\n",
    "Gradient_Boo_preds = Gradient_Boo_model.predict(X_test_std)\n",
    "Random_For_preds = Random_For_model.predict(X_test_std)\n",
    "\n",
    "\n",
    "#calculating accuracy for each model\n",
    "logistic_reg_accuracy = accuracy_score(y_test, logistic_reg_preds)\n",
    "Gradient_Boo_accuracy = accuracy_score(y_test, Gradient_Boo_preds)\n",
    "Random_For_accuracy = accuracy_score(y_test, Random_For_preds)\n",
    "\n",
    "\n",
    "#printing accuracy for each model\n",
    "print(\"Logistic Regression Accuracy: \", logistic_reg_accuracy)\n",
    "print(\"Gradient Boosting Classifier Accuracy: \", Gradient_Boo_accuracy)\n",
    "print(\"Random Forest Classifier Accuracy: \", Random_For_accuracy)\n",
    "\n",
    "\n",
    "#printing classification reports\n",
    "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, logistic_reg_preds, zero_division = 1))\n",
    "print(\"\\nGradient Boosting Classifier Classification Report:\\n\", classification_report(y_test, Gradient_Boo_preds, zero_division = 1))\n",
    "print(\"\\nRandom Forest Classifier Classification Report:\\n\", classification_report(y_test, Random_For_preds, zero_division = 1))\n",
    "\n",
    "\n",
    "\n",
    "#function for plotting confusion matrices for better visualization\n",
    "def plot_cm(confusion_m, model):\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    sns.heatmap(confusion_m, annot = True, fmt = 'd', cmap = 'Blues', annot_kws = {\"size\": 14})\n",
    "    plt.title(f'{model} Confusion Matrix', fontsize = 17)\n",
    "    plt.xlabel('True Prediction', fontsize = 14)\n",
    "    plt.ylabel('Model_Prediction', fontsize = 14)\n",
    "    plt.xticks(ticks = [0.5, 1.5, 2.5, 3.5], labels = ['1', '2', '3', '4'], fontsize = 12)\n",
    "    plt.yticks(ticks = [0.5, 1.5, 2.5, 3.5], labels = ['1', '2', '3', '4'], fontsize = 12)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#printing confusion matrices\n",
    "plot_cm(confusion_matrix(y_test, logistic_reg_preds), 'Logistic Regression')\n",
    "plot_cm(confusion_matrix(y_test, Gradient_Boo_preds), 'Gradient Boosting Classifier')\n",
    "plot_cm(confusion_matrix(y_test, Random_For_preds), 'Random Forest Classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aad5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 6: Analyzing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c815144",
   "metadata": {},
   "source": [
    "The Random Forest Model performed the best and heres why...\n",
    "\n",
    "1. Accuracy Score:\n",
    "based on the Accuracy Scores from above, we can see that the Random Forest model has the highest accuracy. This means that the Random Forest model has the highest proportion of correct predections out of all instances in the data\n",
    "\n",
    "\n",
    "2. Classification Report: \n",
    "so based on the Precision, Recall, and F1-score for individual classes... Random Forest model has the highest Precision for classes 1, 3, and 4. Random Forest model has the highest recall for classes 1, 3, and 4. Random Forest has the highest F1-score for classes 1, 3, and 4.... for class 2, Gradient Boosting model has a higher F1-score and precision than Random Forest, but not by much.\n",
    "\n",
    "The Random Forest model also has a the highest weighted average F1-score. This means that the model has the best balance between precision and recall, when taking into account of the class sizes\n",
    "\n",
    "\n",
    "\n",
    "3. Confusion Matrix: \n",
    "Logistic Regression Model- This model is very biased towards class 2 because it has a high number of true positives (2072), but does poorly when correctly predicting the other classes (1, 3, 4). \n",
    "\n",
    "Also, it has a large number of false positives for class 2, which means that the model tends to incorrectly predict instances as class 2.\n",
    "\n",
    "\n",
    "Gradient Boosting Model - Similarily just like the Logistic Regression Model, this model is also has a bias towards class 2, but it does perfom better than Logistic Regression for classes 3 and 4. However, it still has a slightly high number of false positives for class 2.\n",
    "\n",
    "\n",
    "\n",
    "Random Forest Model - This model also seems to have some bias towards class 2 aswell, but it performs better when correctly predicting for the other classes (1, 3, and 4) compared to the Logistic Regression Model and the Gradient Boosting models. This model is illustrates a more balanced performance across all classes when considereding true positives (correctly predicting the class). When comparing all three models, the Random Forest Model seems to be the least biased and has the better performance across all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9be799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 7: Feature Importance\n",
    "#using the top most important features from the model that performed the best for us (Random Forest)\n",
    "#doing this so that we can identify the most influential factors contributing to the severity of traffic accidents\n",
    "\n",
    "rf_important_features = feature_importances.sort_values(ascending = False)\n",
    "print(\"Top 10 important features from the Random Forest model:\")\n",
    "for index, feature in enumerate(rf_important_features[:10].index):\n",
    "    print(f\"{index + 1}. {feature}: {rf_important_features[feature]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0d1d95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#step 8: visualization of results based on top 10 important features from the Random Forest model\n",
    "\n",
    "#dataframe containing the top 10 important features form the Random Forest model\n",
    "top_ten_featuresDF = normalized_Atl_data[['Pressure(in)', 'Temperature(F)', 'Humidity(%)', 'End_Lng', 'End_Lat', 'Start_Lng', 'Start_Lat', 'Distance(mi)', 'Wind_Speed(mph)', 'Wind_Chill(F)', 'Severity']]\n",
    "\n",
    "#boxplots for each feature against the target variable ('Severity') \n",
    "for feature in top_ten_featuresDF.columns[:-1]:\n",
    "    plt.figure(figsize= (8, 6))\n",
    "    sns.boxplot(x = 'Severity', y = feature, data = top_ten_featuresDF)\n",
    "    plt.title(f'{feature} vs Severity')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb27d236",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
